{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f19fe218-8272-4a78-95dc-b45c7944d26d",
      "metadata": {
        "id": "f19fe218-8272-4a78-95dc-b45c7944d26d"
      },
      "source": [
        "# Build and deploy a diabetes classification model with Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d23538a-e809-4747-9bd4-5610f8544ea1",
      "metadata": {
        "id": "9d23538a-e809-4747-9bd4-5610f8544ea1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee04a090",
      "metadata": {
        "id": "ee04a090"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81be8e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f81be8e9",
        "outputId": "ccd38ea1-b9a3-4086-c1f9-6985eeeb90a1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-bigquery==3.25.0\n",
            "  Downloading google_cloud_bigquery-3.25.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery==3.25.0) (2.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.25.0) (2.37.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.25.0) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.25.0) (2.7.2)\n",
            "Requirement already satisfied: packaging>=20.0.0 in ./.local/lib/python3.10/site-packages (from google-cloud-bigquery==3.25.0) (21.3)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.25.0) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery==3.25.0) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in ./.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery==3.25.0) (1.58.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery==3.25.0) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery==3.25.0) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery==3.25.0) (1.47.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery==3.25.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery==3.25.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery==3.25.0) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==3.25.0) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0.0->google-cloud-bigquery==3.25.0) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery==3.25.0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.25.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.25.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.25.0) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.25.0) (2024.12.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery==3.25.0) (0.6.1)\n",
            "Downloading google_cloud_bigquery-3.25.0-py2.py3-none-any.whl (239 kB)\n",
            "Installing collected packages: google-cloud-bigquery\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 3.4.2\n",
            "    Uninstalling google-cloud-bigquery-3.4.2:\n",
            "      Successfully uninstalled google-cloud-bigquery-3.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.9.0 requires google-cloud-resource-manager>=1.10.3, but you have google-cloud-resource-manager 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-cloud-bigquery-3.25.0\n",
            "Collecting google-cloud-aiplatform==1.59.0\n",
            "  Downloading google_cloud_aiplatform-1.59.0-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (2.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (2.37.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (21.3)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (2.7.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (1.8.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (1.8.5.post1)\n",
            "Requirement already satisfied: pydantic<3 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (1.10.21)\n",
            "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.59.0) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in ./.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (1.58.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (1.47.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.59.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.59.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.59.0) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.59.0) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.59.0) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.59.0) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.59.0) (0.13.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.59.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in ./.local/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform==1.59.0) (4.5.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.59.0) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.59.0) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.59.0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.59.0) (2024.12.14)\n",
            "Downloading google_cloud_aiplatform-1.59.0-py2.py3-none-any.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-aiplatform\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 1.22.0\n",
            "    Uninstalling google-cloud-aiplatform-1.22.0:\n",
            "      Successfully uninstalled google-cloud-aiplatform-1.22.0\n",
            "Successfully installed google-cloud-aiplatform-1.59.0\n",
            "Found existing installation: Shapely 1.8.5.post1\n",
            "Uninstalling Shapely-1.8.5.post1:\n",
            "  Successfully uninstalled Shapely-1.8.5.post1\n",
            "\u001b[33mWARNING: Skipping pygeos as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: geopandas 1.0.1\n",
            "Uninstalling geopandas-1.0.1:\n",
            "  Successfully uninstalled geopandas-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.9.0 requires google-cloud-resource-manager>=1.10.3, but you have google-cloud-resource-manager 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pydot\n",
            "  Downloading pydot-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pyparsing>=3.0.9 in /opt/conda/lib/python3.10/site-packages (from pydot) (3.2.0)\n",
            "Downloading pydot-3.0.4-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: pydot\n",
            "Successfully installed pydot-3.0.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-liberation libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin libgvc6\n",
            "  libgvpr2 liblab-gamut1 libpathplan4\n",
            "Suggested packages:\n",
            "  gsfonts graphviz-doc\n",
            "The following NEW packages will be installed:\n",
            "  fonts-liberation graphviz libann0 libcdt5 libcgraph6 libgts-0.7-5 libgts-bin\n",
            "  libgvc6 libgvpr2 liblab-gamut1 libpathplan4\n",
            "0 upgraded, 11 newly installed, 0 to remove and 2 not upgraded.\n",
            "Need to get 3034 kB of archives.\n",
            "After this operation, 11.5 MB of additional disk space will be used.\n",
            "Get:1 https://deb.debian.org/debian bullseye/main amd64 fonts-liberation all 1:1.07.4-11 [828 kB]\n",
            "Get:2 https://deb.debian.org/debian bullseye/main amd64 libann0 amd64 1.1.2+doc-7 [25.3 kB]\n",
            "Get:3 https://deb.debian.org/debian bullseye/main amd64 libcdt5 amd64 2.42.2-5+deb11u1 [62.2 kB]\n",
            "Get:4 https://deb.debian.org/debian bullseye/main amd64 libcgraph6 amd64 2.42.2-5+deb11u1 [85.5 kB]\n",
            "Get:5 https://deb.debian.org/debian bullseye/main amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4+b1 [158 kB]\n",
            "Get:6 https://deb.debian.org/debian bullseye/main amd64 libpathplan4 amd64 2.42.2-5+deb11u1 [64.3 kB]\n",
            "Get:7 https://deb.debian.org/debian bullseye/main amd64 libgvc6 amd64 2.42.2-5+deb11u1 [695 kB]\n",
            "Get:8 https://deb.debian.org/debian bullseye/main amd64 libgvpr2 amd64 2.42.2-5+deb11u1 [212 kB]\n",
            "Get:9 https://deb.debian.org/debian bullseye/main amd64 liblab-gamut1 amd64 2.42.2-5+deb11u1 [221 kB]\n",
            "Get:10 https://deb.debian.org/debian bullseye/main amd64 graphviz amd64 2.42.2-5+deb11u1 [632 kB]\n",
            "Get:11 https://deb.debian.org/debian bullseye/main amd64 libgts-bin amd64 0.7.6+darcs121130-4+b1 [50.3 kB]\n",
            "Fetched 3034 kB in 0s (17.0 MB/s)       \u001b[0m\u001b[33m\n",
            "\n",
            "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package fonts-liberation.\n",
            "(Reading database ... 140326 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-liberation_1%3a1.07.4-11_all.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking fonts-liberation (1:1.07.4-11) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libann0.\n",
            "Preparing to unpack .../01-libann0_1.1.2+doc-7_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libann0 (1.1.2+doc-7) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libcdt5:amd64.\n",
            "Preparing to unpack .../02-libcdt5_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libcdt5:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libcgraph6:amd64.\n",
            "Preparing to unpack .../03-libcgraph6_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libcgraph6:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libgts-0.7-5:amd64.\n",
            "Preparing to unpack .../04-libgts-0.7-5_0.7.6+darcs121130-4+b1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4+b1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libpathplan4:amd64.\n",
            "Preparing to unpack .../05-libpathplan4_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libpathplan4:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libgvc6.\n",
            "Preparing to unpack .../06-libgvc6_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libgvc6 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libgvpr2:amd64.\n",
            "Preparing to unpack .../07-libgvpr2_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libgvpr2:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package liblab-gamut1:amd64.\n",
            "Preparing to unpack .../08-liblab-gamut1_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking liblab-gamut1:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package graphviz.\n",
            "Preparing to unpack .../09-graphviz_2.42.2-5+deb11u1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking graphviz (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libgts-bin.\n",
            "Preparing to unpack .../10-libgts-bin_0.7.6+darcs121130-4+b1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libgts-bin (0.7.6+darcs121130-4+b1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up liblab-gamut1:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4+b1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libpathplan4:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libann0 (1.1.2+doc-7) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up fonts-liberation (1:1.07.4-11) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [#######################################...................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libcdt5:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libcgraph6:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libgts-bin (0.7.6+darcs121130-4+b1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libgvc6 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libgvpr2:amd64 (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up graphviz (2.42.2-5+deb11u1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for libc-bin (2.31-13+deb11u11) ...\n",
            "ldconfig: /lib/libnvonnxparser.so.8 is not a symbolic link\n",
            "\n",
            "ldconfig: /lib/libnvinfer_vc_plugin.so.8 is not a symbolic link\n",
            "\n",
            "ldconfig: /lib/libnvinfer_plugin.so.8 is not a symbolic link\n",
            "\n",
            "ldconfig: /lib/libnvinfer_dispatch.so.8 is not a symbolic link\n",
            "\n",
            "ldconfig: /lib/libnvinfer.so.8 is not a symbolic link\n",
            "\n",
            "ldconfig: /lib/libnvparsers.so.8 is not a symbolic link\n",
            "\n",
            "ldconfig: /lib/libnvinfer_lean.so.8 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.9.4-2) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2) ...\n",
            "\n",
            "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
          ]
        }
      ],
      "source": [
        "!pip3 install google-cloud-bigquery==3.25.0 -U\n",
        "!pip install google-cloud-aiplatform==1.59.0\n",
        "!pip uninstall -y shapely pygeos geopandas\n",
        "!pip install shapely==1.8.5.post1 pygeos==0.12.0 geopandas>=0.12.2\n",
        "# Install pydot and graphviz\n",
        "!pip install pydot\n",
        "!sudo apt install graphviz -y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696f3796",
      "metadata": {
        "id": "696f3796"
      },
      "source": [
        "### Restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da06064f",
      "metadata": {
        "id": "da06064f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc37a668",
      "metadata": {
        "id": "fc37a668"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ab494b",
      "metadata": {
        "id": "c1ab494b",
        "tags": [],
        "outputId": "da7f8265-acea-4ef5-ece5-806aa47ecd6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
          ]
        }
      ],
      "source": [
        "# Add installed library dependencies to Python PATH variable.\n",
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f68df5dd-c456-4edd-8f58-71597f10c0ae",
      "metadata": {
        "id": "f68df5dd-c456-4edd-8f58-71597f10c0ae",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
        "PROJECT_ID = \"qwiklabs-gcp-02-816e025829d7\"\n",
        "REGION = \"us-central1\"\n",
        "GCS_BUCKET = f\"gs://{PROJECT_ID}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4931ae91-3ba1-437a-9c37-187a41a3d227",
      "metadata": {
        "id": "4931ae91-3ba1-437a-9c37-187a41a3d227",
        "outputId": "5552f5bc-3a20-441f-aa9f-a3269b6dba36",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating gs://qwiklabs-gcp-02-816e025829d7/...\n"
          ]
        }
      ],
      "source": [
        "!gcloud storage buckets create -l $REGION $GCS_BUCKET"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ebbc2b-21ad-47f0-829f-9beba0deba9d",
      "metadata": {
        "id": "d3ebbc2b-21ad-47f0-829f-9beba0deba9d"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf558fc-d0fc-4452-8281-7d7cd0cffe50",
      "metadata": {
        "id": "0bf558fc-d0fc-4452-8281-7d7cd0cffe50",
        "tags": [],
        "outputId": "c19317d1-8601-4104-c09b-bc501f5bf8cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-20 11:35:17.227353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "# TensorFlow model building libraries.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Libraries for data and plot model training metrics.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Import the Vertex AI Python SDK.\n",
        "from google.cloud import aiplatform as vertexai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d296167a-13b9-4895-be8b-b3b49fad5d47",
      "metadata": {
        "id": "d296167a-13b9-4895-be8b-b3b49fad5d47"
      },
      "source": [
        "### Initialize Vertex AI Python SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c178b0-0edb-4e4b-abb4-d3cc0bd676de",
      "metadata": {
        "id": "34c178b0-0edb-4e4b-abb4-d3cc0bd676de"
      },
      "source": [
        "Initialize the Vertex AI Python SDK with GCP Project, Region, and Google Cloud Storage Bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a43371e-2c64-4a76-8698-fa768043dbdd",
      "metadata": {
        "id": "3a43371e-2c64-4a76-8698-fa768043dbdd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2917411-811c-46dd-8eda-e8ef579c568d",
      "metadata": {
        "id": "d2917411-811c-46dd-8eda-e8ef579c568d"
      },
      "source": [
        "## Build and train the model locally in a Vertex Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef491df4-c35f-4555-a6b6-96114c3d3c6e",
      "metadata": {
        "id": "ef491df4-c35f-4555-a6b6-96114c3d3c6e"
      },
      "source": [
        "### Load the dataset and read it into a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee70d2c-c0e3-4c75-9bc6-b42dad6c7267",
      "metadata": {
        "id": "2ee70d2c-c0e3-4c75-9bc6-b42dad6c7267",
        "tags": []
      },
      "outputs": [],
      "source": [
        "csv_file = 'data/diabetes_prediction_dataset.csv'\n",
        "dataframe = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42d5ea91",
      "metadata": {
        "id": "42d5ea91"
      },
      "source": [
        "Inspect the dataset by checking the first five rows of the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e2b54f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "96e2b54f",
        "outputId": "51320b11-03aa-4196-d5f6-521486decf70",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>smoking_history</th>\n",
              "      <th>bmi</th>\n",
              "      <th>HbA1c_level</th>\n",
              "      <th>blood_glucose_level</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>never</td>\n",
              "      <td>25.19</td>\n",
              "      <td>6.6</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Info</td>\n",
              "      <td>27.32</td>\n",
              "      <td>6.6</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>never</td>\n",
              "      <td>27.32</td>\n",
              "      <td>5.7</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Female</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>current</td>\n",
              "      <td>23.45</td>\n",
              "      <td>5.0</td>\n",
              "      <td>155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>76.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>current</td>\n",
              "      <td>20.14</td>\n",
              "      <td>4.8</td>\n",
              "      <td>155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
              "0  Female  80.0             0              1           never  25.19   \n",
              "1  Female  54.0             0              0         No Info  27.32   \n",
              "2    Male  28.0             0              0           never  27.32   \n",
              "3  Female  36.0             0              0         current  23.45   \n",
              "4    Male  76.0             1              1         current  20.14   \n",
              "\n",
              "   HbA1c_level  blood_glucose_level  diabetes  \n",
              "0          6.6                  140         0  \n",
              "1          6.6                   80         0  \n",
              "2          5.7                  158         0  \n",
              "3          5.0                  155         0  \n",
              "4          4.8                  155         0  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcfd827d",
      "metadata": {
        "id": "dcfd827d"
      },
      "source": [
        "Create the target variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7850ec8",
      "metadata": {
        "id": "b7850ec8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataframe['target'] = dataframe.pop('diabetes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23f8b2d",
      "metadata": {
        "id": "d23f8b2d"
      },
      "source": [
        "### Split the DataFrame into training, validation, and test sets\n",
        "\n",
        "The dataset is in a single pandas DataFrame. Split it into training, validation, and test sets using a, for example, 80:10:10 ratio, respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6bec53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da6bec53",
        "outputId": "8756e282-cd63-46d2-94d1-a9d1a0508789",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "train, val, test = np.split(dataframe.sample(frac=1), [int(0.8*len(dataframe)), int(0.9*len(dataframe))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c7b659",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28c7b659",
        "outputId": "b02d38e8-eeeb-484b-b335-0e058c164f59",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80000 training examples\n",
            "10000 validation examples\n",
            "10000 test examples\n"
          ]
        }
      ],
      "source": [
        "print(len(train), 'training examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73b8bfee",
      "metadata": {
        "id": "73b8bfee"
      },
      "source": [
        "### Create an input pipeline using tf.data\n",
        "\n",
        "Next, create a utility function that converts each training, validation, and test set DataFrame into a `tf.data.Dataset`, then shuffles and batches the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695cc0cb",
      "metadata": {
        "id": "695cc0cb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=256):\n",
        "  df = dataframe.copy()\n",
        "  labels = df.pop('target')\n",
        "  df = {key: value.to_numpy()[:,tf.newaxis] for key, value in dataframe.items()}\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b11186f4",
      "metadata": {
        "id": "b11186f4"
      },
      "source": [
        "### Apply the Keras preprocessing layers\n",
        "\n",
        "The Keras preprocessing layers allow you to build Keras-native input processing pipelines, which can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\n",
        "\n",
        "In this tutorial, you will use the following four preprocessing layers to demonstrate how to perform preprocessing, structured data encoding, and feature engineering:\n",
        "\n",
        "- `tf.keras.layers.Normalization`: Performs feature-wise normalization of input features.\n",
        "- `tf.keras.layers.CategoryEncoding`: Turns integer categorical features into one-hot, multi-hot, or <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" class=\"external\">tf-idf</a>\n",
        "dense representations.\n",
        "- `tf.keras.layers.StringLookup`: Turns string categorical values into integer indices.\n",
        "- `tf.keras.layers.IntegerLookup`: Turns integer categorical values into integer indices.\n",
        "\n",
        "You can learn more about the available layers in the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide.\n",
        "\n",
        "- For _numerical features_ of the dataset, you will use a `tf.keras.layers.Normalization` layer to standardize the distribution of the data.\n",
        "- For _categorical features_, you will transform them to multi-hot encoded tensors with `tf.keras.layers.CategoryEncoding`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "994509d9",
      "metadata": {
        "id": "994509d9"
      },
      "source": [
        "### Numerical columns\n",
        "\n",
        "Define a new utility function that returns a layer which applies feature-wise normalization to numerical features using that Keras preprocessing layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ddbf66",
      "metadata": {
        "id": "50ddbf66",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_normalization_layer(name, dataset):\n",
        "  # Create a Normalization layer for the feature.\n",
        "  normalizer = layers.Normalization(axis=None)\n",
        "\n",
        "  # Prepare a Dataset that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the statistics of the data.\n",
        "  normalizer.adapt(feature_ds)\n",
        "\n",
        "  return normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98901150",
      "metadata": {
        "id": "98901150"
      },
      "source": [
        "### Categorical columns\n",
        "\n",
        "Define another new utility function that returns a layer which maps values from a vocabulary to integer indices and multi-hot encodes the features using the `tf.keras.layers.StringLookup`, `tf.keras.layers.IntegerLookup`, and `tf.keras.CategoryEncoding` preprocessing layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b44921",
      "metadata": {
        "id": "87b44921",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "  # Create a layer that turns strings into integer indices.\n",
        "  if dtype == 'string':\n",
        "    index = layers.StringLookup(max_tokens=max_tokens)\n",
        "  # Otherwise, create a layer that turns integer values into integer indices.\n",
        "  else:\n",
        "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
        "\n",
        "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
        "  feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "  # Learn the set of possible values and assign them a fixed integer index.\n",
        "  index.adapt(feature_ds)\n",
        "\n",
        "  # Encode the integer indices.\n",
        "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
        "\n",
        "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
        "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
        "  return lambda feature: encoder(index(feature))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f3f140",
      "metadata": {
        "id": "55f3f140"
      },
      "source": [
        "### Preprocess selected features to train the model on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefe4411",
      "metadata": {
        "id": "eefe4411",
        "tags": []
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c585a234",
      "metadata": {
        "id": "c585a234"
      },
      "source": [
        "Normalize the numerical features, and add them to one list of inputs called `encoded_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3bfa25a",
      "metadata": {
        "id": "d3bfa25a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "all_inputs = {}\n",
        "encoded_features = []\n",
        "\n",
        "# Numerical features.\n",
        "for header in ['age','bmi','HbA1c_level','blood_glucose_level']:\n",
        "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "  normalization_layer = get_normalization_layer(header, train_ds)\n",
        "  encoded_numeric_col = normalization_layer(numeric_col)\n",
        "  all_inputs[header] = numeric_col\n",
        "  encoded_features.append(encoded_numeric_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a39f24",
      "metadata": {
        "id": "60a39f24"
      },
      "source": [
        "Turn the integer categorical values from the dataset into integer indices, perform multi-hot encoding, and add the resulting feature inputs to `encoded_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7b1053",
      "metadata": {
        "id": "3d7b1053",
        "tags": []
      },
      "outputs": [],
      "source": [
        "int_categorical_cols = ['hypertension', 'heart_disease']\n",
        "\n",
        "for header in int_categorical_cols:\n",
        "  int_categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
        "  encoding_layer = get_category_encoding_layer(name=header,\n",
        "                                             dataset=train_ds,\n",
        "                                             dtype='int64',\n",
        "                                             max_tokens=5)\n",
        "  encoded_int_categorical_col = encoding_layer(int_categorical_col)\n",
        "  all_inputs[header] = int_categorical_col\n",
        "  encoded_features.append(encoded_int_categorical_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e627fa63",
      "metadata": {
        "id": "e627fa63"
      },
      "source": [
        "Repeat the same step for the string categorical values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c23c94",
      "metadata": {
        "id": "13c23c94",
        "tags": []
      },
      "outputs": [],
      "source": [
        "categorical_cols = ['gender', 'smoking_history']\n",
        "\n",
        "for header in categorical_cols:\n",
        "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "  encoding_layer = get_category_encoding_layer(name=header,\n",
        "                                               dataset=train_ds,\n",
        "                                               dtype='string',\n",
        "                                               max_tokens=5)\n",
        "  encoded_categorical_col = encoding_layer(categorical_col)\n",
        "  all_inputs[header] = categorical_col\n",
        "  encoded_features.append(encoded_categorical_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mrJdAmNvpcEw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrJdAmNvpcEw",
        "outputId": "54cf9112-3c2a-4811-9107-f5bbb3030567",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_1')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_2')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_3')>, <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'category_encoding')>, <KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'category_encoding_1')>, <KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'category_encoding_2')>, <KerasTensor: shape=(None, 5) dtype=float32 (created by layer 'category_encoding_3')>]\n"
          ]
        }
      ],
      "source": [
        "print(encoded_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc33f2c3",
      "metadata": {
        "id": "cc33f2c3"
      },
      "source": [
        "### Create, compile, and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87696fea",
      "metadata": {
        "id": "87696fea",
        "tags": []
      },
      "outputs": [],
      "source": [
        "all_features = tf.keras.layers.concatenate(encoded_features)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "model = tf.keras.Model(all_inputs, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b076ecb",
      "metadata": {
        "id": "9b076ecb"
      },
      "source": [
        "Configure the model with Keras `Model.compile`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131fa95a",
      "metadata": {
        "id": "131fa95a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"],\n",
        "              run_eagerly=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3260b8",
      "metadata": {
        "id": "7f3260b8"
      },
      "source": [
        "Next, train and test the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40665b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e40665b5",
        "outputId": "769682db-f0da-4a9a-cf13-c598328bb27b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:639: UserWarning: Input dict contained keys ['target'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 14s 41ms/step - loss: 0.3161 - accuracy: 0.8903 - val_loss: 0.1320 - val_accuracy: 0.9490\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.1402 - accuracy: 0.9515 - val_loss: 0.1199 - val_accuracy: 0.9583\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 14s 42ms/step - loss: 0.1299 - accuracy: 0.9560 - val_loss: 0.1184 - val_accuracy: 0.9594\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 13s 40ms/step - loss: 0.1261 - accuracy: 0.9577 - val_loss: 0.1173 - val_accuracy: 0.9598\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.1227 - accuracy: 0.9588 - val_loss: 0.1157 - val_accuracy: 0.9607\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 15s 45ms/step - loss: 0.1206 - accuracy: 0.9592 - val_loss: 0.1143 - val_accuracy: 0.9616\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.1187 - accuracy: 0.9598 - val_loss: 0.1132 - val_accuracy: 0.9624\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.1166 - accuracy: 0.9614 - val_loss: 0.1115 - val_accuracy: 0.9630\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 13s 40ms/step - loss: 0.1147 - accuracy: 0.9615 - val_loss: 0.1099 - val_accuracy: 0.9638\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.1121 - accuracy: 0.9620 - val_loss: 0.1076 - val_accuracy: 0.9641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f6ab8447d90>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2f703c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed2f703c",
        "outputId": "bc99fc7e-f8fb-433e-de41-b9ccfb80b918",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0988 - accuracy: 0.9662\n",
            "{'loss': 0.09882796555757523, 'accuracy': 0.9661999940872192}\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(test_ds, return_dict=True)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fc3fed-aa4c-40b2-8c44-19be21ba4689",
      "metadata": {
        "id": "f4fc3fed-aa4c-40b2-8c44-19be21ba4689"
      },
      "source": [
        "## Containerize the model code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3905338-288b-4565-9abb-9053d7559315",
      "metadata": {
        "id": "f3905338-288b-4565-9abb-9053d7559315"
      },
      "source": [
        "Next step is to train and deploy the model on Google Cloud's Vertex AI platform."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a033e3c3-9dad-49d8-b53c-fd48113a8f90",
      "metadata": {
        "id": "a033e3c3-9dad-49d8-b53c-fd48113a8f90"
      },
      "source": [
        "### 1. Write a `model.py` training script\n",
        "\n",
        "First, tidy up the local TensorFlow model training code from above into a training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0129184d-15ed-4ebe-bbdc-6c2687eb18cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0129184d-15ed-4ebe-bbdc-6c2687eb18cf",
        "outputId": "404f967e-8e6f-4fdd-f8e9-8b89e70577d7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created directory: diabetes-prediction-model\n",
            "Created directory: diabetes-prediction-model/trainer\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR = \"diabetes-prediction-model\"\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "    print(f\"Created directory: {MODEL_DIR}\")\n",
        "trainer_dir = os.path.join(MODEL_DIR, \"trainer\")\n",
        "if not os.path.exists(trainer_dir):\n",
        "    os.makedirs(trainer_dir)\n",
        "    print(f\"Created directory: {trainer_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2594afe7-b9e0-4957-9156-d2e595fde62f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2594afe7-b9e0-4957-9156-d2e595fde62f",
        "outputId": "cb5b98c1-1ce5-4d6e-f8e9-9873faa8e547",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing diabetes-prediction-model/trainer/model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {MODEL_DIR}/trainer/model.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "    \"\"\"Convert a Pandas DataFrame into a TensorFlow Dataset.\"\"\"\n",
        "    df = dataframe.copy()\n",
        "    labels = df.pop('target')\n",
        "    # Convert each feature column to (batch, 1) shape\n",
        "    df = {key: value.to_numpy()[:, tf.newaxis] for key, value in df.items()}\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(batch_size)\n",
        "    return ds\n",
        "\n",
        "def get_normalization_layer(name, dataset):\n",
        "    \"\"\"Create and adapt a normalization layer for a numerical feature.\"\"\"\n",
        "    normalizer = layers.Normalization(axis=None)\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])  # Extract a single feature\n",
        "    normalizer.adapt(feature_ds)\n",
        "    return normalizer\n",
        "\n",
        "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
        "    \"\"\"Create and adapt a category encoding layer for a categorical feature.\"\"\"\n",
        "    # Create either a StringLookup or IntegerLookup based on dtype\n",
        "    if dtype == 'string':\n",
        "        index = layers.StringLookup(max_tokens=max_tokens)\n",
        "    else:\n",
        "        index = layers.IntegerLookup(max_tokens=max_tokens)\n",
        "\n",
        "    # Adapt the index layer to the dataset\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    index.adapt(feature_ds)\n",
        "\n",
        "    # Create a one-hot (multi-hot) encoding layer\n",
        "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
        "    return lambda feature: encoder(index(feature))\n",
        "\n",
        "def load_datasets(csv_file, batch_size=32):\n",
        "    \"\"\"\n",
        "    Load the CSV file and split it into train, validation, and test sets.\n",
        "    Returns tf.data.Datasets for each split.\n",
        "    \"\"\"\n",
        "    dataframe = pd.read_csv(csv_file)\n",
        "\n",
        "    # Rename 'diabetes' column to 'target'\n",
        "    dataframe['target'] = dataframe.pop('diabetes')\n",
        "\n",
        "    # Randomly shuffle and split data into train/val/test\n",
        "    train, val, test = np.split(\n",
        "        dataframe.sample(frac=1),\n",
        "        [int(0.8 * len(dataframe)), int(0.9 * len(dataframe))]\n",
        "    )\n",
        "\n",
        "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "def build_feature_layers(train_ds):\n",
        "    \"\"\"\n",
        "    Build input layers and corresponding preprocessing/encoding layers for each feature.\n",
        "    Returns (dict_of_inputs, list_of_encoded_features).\n",
        "    \"\"\"\n",
        "    all_inputs = {}\n",
        "    encoded_features = []\n",
        "\n",
        "    # Numerical features\n",
        "    numeric_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "    for header in numeric_features:\n",
        "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
        "        normalization_layer = get_normalization_layer(header, train_ds)\n",
        "        encoded_numeric_col = normalization_layer(numeric_col)\n",
        "        all_inputs[header] = numeric_col\n",
        "        encoded_features.append(encoded_numeric_col)\n",
        "\n",
        "    # Integer categorical features\n",
        "    int_categorical_cols = ['hypertension', 'heart_disease']\n",
        "    for header in int_categorical_cols:\n",
        "        int_categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int32')\n",
        "        encoding_layer = get_category_encoding_layer(\n",
        "            name=header, dataset=train_ds, dtype='int', max_tokens=5\n",
        "        )\n",
        "        encoded_int_categorical_col = encoding_layer(int_categorical_col)\n",
        "        all_inputs[header] = int_categorical_col\n",
        "        encoded_features.append(encoded_int_categorical_col)\n",
        "\n",
        "    # String categorical features\n",
        "    categorical_cols = ['gender', 'smoking_history']\n",
        "    for header in categorical_cols:\n",
        "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
        "        encoding_layer = get_category_encoding_layer(\n",
        "            name=header, dataset=train_ds, dtype='string', max_tokens=5\n",
        "        )\n",
        "        encoded_categorical_col = encoding_layer(categorical_col)\n",
        "        all_inputs[header] = categorical_col\n",
        "        encoded_features.append(encoded_categorical_col)\n",
        "\n",
        "    return all_inputs, encoded_features\n",
        "\n",
        "def build_model(all_inputs, encoded_features, dropout_rate=0.5):\n",
        "    \"\"\"\n",
        "    Build and compile the TensorFlow model.\n",
        "    \"\"\"\n",
        "    # Concatenate all encoded features\n",
        "    all_features = layers.concatenate(encoded_features)\n",
        "    x = layers.Dense(32, activation=\"relu\")(all_features)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=all_inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_evaluate(hparams):\n",
        "    \"\"\"\n",
        "    Main training procedure: loads datasets, builds a model, trains, evaluates,\n",
        "    and exports the model in SavedModel format.\n",
        "\n",
        "    hparams(dict) is expected to have the following keys:\n",
        "      - 'data-file': path to the CSV file\n",
        "      - 'batch-size': batch size\n",
        "      - 'epochs': number of epochs\n",
        "      - 'dropout': dropout rate\n",
        "      - 'model-dir': path where the model will be exported\n",
        "    \"\"\"\n",
        "    # 1. Load the data\n",
        "    csv_file = hparams['data-file']\n",
        "    train_ds, val_ds, test_ds = load_datasets(csv_file, batch_size=hparams['batch-size'])\n",
        "\n",
        "    # 2. Build feature layers and the model\n",
        "    all_inputs, encoded_features = build_feature_layers(train_ds)\n",
        "    model = build_model(all_inputs, encoded_features, dropout_rate=hparams['dropout'])\n",
        "\n",
        "    # 3. Train the model\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=hparams['epochs']\n",
        "    )\n",
        "\n",
        "    # 4. Evaluate on the test set\n",
        "    test_result = model.evaluate(test_ds, return_dict=True)\n",
        "    print(\"Test evaluation:\", test_result)\n",
        "\n",
        "    # 5. Export the model\n",
        "    export_dir = hparams['model-dir']\n",
        "    model.save(export_dir)\n",
        "    print(f\"Model exported to: {export_dir}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e16b936-d93a-4411-a494-aacda93b05f4",
      "metadata": {
        "id": "1e16b936-d93a-4411-a494-aacda93b05f4"
      },
      "source": [
        "### 2. Write a `task.py` file as an entrypoint to the custom model container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17517e0b-a2ac-489a-bf03-357ace5d6577",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17517e0b-a2ac-489a-bf03-357ace5d6577",
        "outputId": "4b5f24a5-5e97-4c72-e3e5-d52b5e97c43a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing diabetes-prediction-model/trainer/task.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {MODEL_DIR}/trainer/task.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from trainer import model  # Assume model.py is under trainer/ directory\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Vertex AI sets the model artifact path in AIP_MODEL_DIR,\n",
        "    # but you can also provide a custom default or override it.\n",
        "    parser.add_argument('--model-dir', dest='model-dir',\n",
        "                        default=os.environ.get('AIP_MODEL_DIR', 'exported_model'),\n",
        "                        type=str,\n",
        "                        help='Path to export the trained model.')\n",
        "\n",
        "    parser.add_argument('--data-file', dest='data-file',\n",
        "                        default='data/diabetes_prediction_dataset.csv',\n",
        "                        type=str,\n",
        "                        help='Path to the CSV dataset file.')\n",
        "\n",
        "    parser.add_argument('--batch-size', dest='batch-size',\n",
        "                        default=32, type=int,\n",
        "                        help='Batch size for training.')\n",
        "\n",
        "    parser.add_argument('--epochs', dest='epochs',\n",
        "                        default=5, type=int,\n",
        "                        help='Number of training epochs.')\n",
        "\n",
        "    parser.add_argument('--dropout', dest='dropout',\n",
        "                        default=0.5, type=float,\n",
        "                        help='Dropout rate.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    hparams = args.__dict__\n",
        "\n",
        "    model.train_evaluate(hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d503a04a-5f15-4503-91e9-1acc4353fd08",
      "metadata": {
        "id": "d503a04a-5f15-4503-91e9-1acc4353fd08"
      },
      "source": [
        "### 3. Write a `Dockerfile` for the custom model container"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253b0320-a3c2-4fbd-96f0-48ca3b49d485",
      "metadata": {
        "id": "253b0320-a3c2-4fbd-96f0-48ca3b49d485"
      },
      "source": [
        "The `Dockerfile` contains instructions to package the model code in `bert-sentiment-classifier` as well as specifies the model code's dependencies needed for execution together in a Docker container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86ede10-6372-4320-89d3-264d4d1b1ca1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86ede10-6372-4320-89d3-264d4d1b1ca1",
        "outputId": "e55822b2-bb8f-4049-ecd8-c5110f4dc185",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing diabetes-prediction-model/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile {MODEL_DIR}/Dockerfile\n",
        "# Specifies base image and tag.\n",
        "# https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
        "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
        "\n",
        "# Sets the container working directory.\n",
        "WORKDIR /root\n",
        "\n",
        "# Copies the requirements.txt into the container to reduce network calls.\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Installs additional packages.\n",
        "RUN pip3 install -U -r requirements.txt\n",
        "\n",
        "# b/203105209 Removes unneeded file from TF2.5 CPU image for python_module CustomJob training.\n",
        "# Will be removed on subsequent public Vertex images.\n",
        "RUN rm -rf /var/sitecustomize/sitecustomize.py\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY . /trainer\n",
        "\n",
        "# Sets the container working directory.\n",
        "WORKDIR /trainer\n",
        "\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2974866-46f7-4f16-b6c0-9ea420ea6d73",
      "metadata": {
        "id": "f2974866-46f7-4f16-b6c0-9ea420ea6d73"
      },
      "source": [
        "### 4. Write a `requirements.txt` file to specify additional ML code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7619e1-fff9-4a47-90a8-3ba9b55e74c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe7619e1-fff9-4a47-90a8-3ba9b55e74c7",
        "outputId": "54f78964-e376-44ef-a0cb-7717422e9bdc",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing diabetes-prediction-model/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile {MODEL_DIR}/requirements.txt\n",
        "tf-models-official==2.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81292584-7a08-4c92-a9ba-e3dbc7005130",
      "metadata": {
        "id": "81292584-7a08-4c92-a9ba-e3dbc7005130"
      },
      "source": [
        "## Use Cloud Build to build and submit the model container to Artifact Registry"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d47400c-aa7c-4929-a584-f5067bd682eb",
      "metadata": {
        "id": "6d47400c-aa7c-4929-a584-f5067bd682eb"
      },
      "source": [
        "Next, [Cloud Build](https://cloud.google.com/build) is used to build and upload the custom TensorFlow model container to [Google Cloud Artifact Registry](https://cloud.google.com/artifact-registry).\n",
        "\n",
        "Cloud Build brings reusability and automation to ML experimentation by enabling building, testing, and deploying ML model code as part of a CI/CD workflow. Artifact Registry provides a centralized repository to store, manage, and secure ML container images. This will allow users to securely share ML work with others and reproduce experiment results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860c0d02-200f-4cc3-bdfd-ba96d233ecc4",
      "metadata": {
        "id": "860c0d02-200f-4cc3-bdfd-ba96d233ecc4"
      },
      "source": [
        "### 1. Create Artifact Registry for custom container images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9918475-f6dc-4fa3-8249-47976e68f529",
      "metadata": {
        "id": "f9918475-f6dc-4fa3-8249-47976e68f529",
        "tags": []
      },
      "outputs": [],
      "source": [
        "ARTIFACT_REGISTRY=\"diabetes-prediction-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d9566d-c6c4-48c8-b4f8-ad239e5ae349",
      "metadata": {
        "id": "93d9566d-c6c4-48c8-b4f8-ad239e5ae349",
        "outputId": "a2a9281b-0063-44e7-8d71-9374f1a91934",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create request issued for: [diabetes-prediction-model]\n",
            "Waiting for operation [projects/qwiklabs-gcp-02-816e025829d7/locations/us-centr\n",
            "al1/operations/d7cafca9-f35b-4981-8749-91061351d48b] to complete...done.       \n",
            "Created repository [diabetes-prediction-model].\n"
          ]
        }
      ],
      "source": [
        "# create a Docker Artifact Registry using the gcloud CLI\n",
        "!gcloud artifacts repositories create $ARTIFACT_REGISTRY \\\n",
        "    --repository-format=docker \\\n",
        "    --location=$REGION \\\n",
        "    --description=\"Artifact Registry for the diabetes prediction model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e900832e-de90-4ba1-ba7d-7973a1de9cc1",
      "metadata": {
        "id": "e900832e-de90-4ba1-ba7d-7973a1de9cc1"
      },
      "source": [
        "### 2. Create `cloudbuild.yaml` instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b580619d-957c-409f-ac15-ccbc0bb79a57",
      "metadata": {
        "id": "b580619d-957c-409f-ac15-ccbc0bb79a57",
        "tags": []
      },
      "outputs": [],
      "source": [
        "IMAGE_NAME=\"diabetes-prediction-model\"\n",
        "IMAGE_TAG=\"latest\"\n",
        "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY}/{IMAGE_NAME}:{IMAGE_TAG}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24790970-988e-4694-b8cd-a2d9d500c11b",
      "metadata": {
        "id": "24790970-988e-4694-b8cd-a2d9d500c11b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "cloudbuild_yaml = f\"\"\"steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: [ 'build', '-t', '{IMAGE_URI}', '.' ]\n",
        "images:\n",
        "- '{IMAGE_URI}'\"\"\"\n",
        "\n",
        "with open(f\"{MODEL_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
        "    fp.write(cloudbuild_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c14b9c6-c120-482d-b5ab-c6a4c53bb205",
      "metadata": {
        "id": "9c14b9c6-c120-482d-b5ab-c6a4c53bb205"
      },
      "source": [
        "### 3. Build and submit the container image to Artifact Registry using Cloud Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebf0093-d66e-49c1-8a55-047020735e60",
      "metadata": {
        "id": "3ebf0093-d66e-49c1-8a55-047020735e60",
        "outputId": "a81c40c6-9962-47e1-b183-6ae238cf9d65",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating temporary archive of 5 file(s) totalling 8.2 KiB before compression.\n",
            "Uploading tarball of [diabetes-prediction-model] to [gs://qwiklabs-gcp-02-816e025829d7_cloudbuild/source/1737373366.538596-2525e80fba04411881443e6c12dd98e0.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-02-816e025829d7/locations/global/builds/141100e6-cbfe-4c78-893d-6c57d785d97e].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/141100e6-cbfe-4c78-893d-6c57d785d97e?project=424228142313 ].\n",
            "Waiting for build to complete. Polling interval: 1 second(s).\n",
            "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
            "starting build \"141100e6-cbfe-4c78-893d-6c57d785d97e\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://qwiklabs-gcp-02-816e025829d7_cloudbuild/source/1737373366.538596-2525e80fba04411881443e6c12dd98e0.tgz#1737373367368273\n",
            "Copying gs://qwiklabs-gcp-02-816e025829d7_cloudbuild/source/1737373366.538596-2525e80fba04411881443e6c12dd98e0.tgz#1737373367368273...\n",
            "/ [1 files][  3.2 KiB/  3.2 KiB]                                                \n",
            "Operation completed over 1 objects/3.2 KiB.                                      \n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  13.82kB\n",
            "Step 1/8 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
            "latest: Pulling from vertex-ai/training/tf-cpu.2-11\n",
            "57c139bbda7e: Pulling fs layer\n",
            "b36d8262105c: Pulling fs layer\n",
            "6289371d2bb3: Pulling fs layer\n",
            "481786a11c6e: Pulling fs layer\n",
            "275a246fad1f: Pulling fs layer\n",
            "6391690ddce2: Pulling fs layer\n",
            "bbce0e9b489f: Pulling fs layer\n",
            "a7ec0c7527b6: Pulling fs layer\n",
            "7aff549e4c97: Pulling fs layer\n",
            "4f4fb700ef54: Pulling fs layer\n",
            "e4bff5ce614c: Pulling fs layer\n",
            "2999545bd192: Pulling fs layer\n",
            "bd33b5a5ec4a: Pulling fs layer\n",
            "8fea5fa58717: Pulling fs layer\n",
            "a73bd9480671: Pulling fs layer\n",
            "139c41303dbe: Pulling fs layer\n",
            "d7df78d5fe61: Pulling fs layer\n",
            "7daa1f38b6ab: Pulling fs layer\n",
            "bce9940ec07e: Pulling fs layer\n",
            "01f79f64a163: Pulling fs layer\n",
            "5113ced62272: Pulling fs layer\n",
            "aa0eae780ea1: Pulling fs layer\n",
            "caeb9ac3c484: Pulling fs layer\n",
            "481786a11c6e: Waiting\n",
            "275a246fad1f: Waiting\n",
            "6391690ddce2: Waiting\n",
            "bbce0e9b489f: Waiting\n",
            "a7ec0c7527b6: Waiting\n",
            "7aff549e4c97: Waiting\n",
            "4f4fb700ef54: Waiting\n",
            "e4bff5ce614c: Waiting\n",
            "2999545bd192: Waiting\n",
            "bd33b5a5ec4a: Waiting\n",
            "8fea5fa58717: Waiting\n",
            "a73bd9480671: Waiting\n",
            "139c41303dbe: Waiting\n",
            "d7df78d5fe61: Waiting\n",
            "7daa1f38b6ab: Waiting\n",
            "bce9940ec07e: Waiting\n",
            "01f79f64a163: Waiting\n",
            "5113ced62272: Waiting\n",
            "aa0eae780ea1: Waiting\n",
            "caeb9ac3c484: Waiting\n",
            "b36d8262105c: Verifying Checksum\n",
            "b36d8262105c: Download complete\n",
            "6289371d2bb3: Verifying Checksum\n",
            "6289371d2bb3: Download complete\n",
            "275a246fad1f: Verifying Checksum\n",
            "275a246fad1f: Download complete\n",
            "57c139bbda7e: Verifying Checksum\n",
            "57c139bbda7e: Download complete\n",
            "bbce0e9b489f: Verifying Checksum\n",
            "bbce0e9b489f: Download complete\n",
            "481786a11c6e: Verifying Checksum\n",
            "481786a11c6e: Download complete\n",
            "6391690ddce2: Verifying Checksum\n",
            "6391690ddce2: Download complete\n",
            "4f4fb700ef54: Verifying Checksum\n",
            "4f4fb700ef54: Download complete\n",
            "a7ec0c7527b6: Verifying Checksum\n",
            "a7ec0c7527b6: Download complete\n",
            "e4bff5ce614c: Verifying Checksum\n",
            "e4bff5ce614c: Download complete\n",
            "2999545bd192: Verifying Checksum\n",
            "2999545bd192: Download complete\n",
            "bd33b5a5ec4a: Verifying Checksum\n",
            "bd33b5a5ec4a: Download complete\n",
            "a73bd9480671: Verifying Checksum\n",
            "a73bd9480671: Download complete\n",
            "8fea5fa58717: Verifying Checksum\n",
            "8fea5fa58717: Download complete\n",
            "d7df78d5fe61: Download complete\n",
            "7daa1f38b6ab: Verifying Checksum\n",
            "7daa1f38b6ab: Download complete\n",
            "bce9940ec07e: Verifying Checksum\n",
            "bce9940ec07e: Download complete\n",
            "01f79f64a163: Verifying Checksum\n",
            "01f79f64a163: Download complete\n",
            "5113ced62272: Verifying Checksum\n",
            "5113ced62272: Download complete\n",
            "aa0eae780ea1: Verifying Checksum\n",
            "aa0eae780ea1: Download complete\n",
            "caeb9ac3c484: Verifying Checksum\n",
            "caeb9ac3c484: Download complete\n",
            "57c139bbda7e: Pull complete\n",
            "b36d8262105c: Pull complete\n",
            "6289371d2bb3: Pull complete\n",
            "139c41303dbe: Verifying Checksum\n",
            "139c41303dbe: Download complete\n",
            "7aff549e4c97: Verifying Checksum\n",
            "7aff549e4c97: Download complete\n",
            "481786a11c6e: Pull complete\n",
            "275a246fad1f: Pull complete\n",
            "6391690ddce2: Pull complete\n",
            "bbce0e9b489f: Pull complete\n",
            "a7ec0c7527b6: Pull complete\n",
            "7aff549e4c97: Pull complete\n",
            "4f4fb700ef54: Pull complete\n",
            "e4bff5ce614c: Pull complete\n",
            "2999545bd192: Pull complete\n",
            "bd33b5a5ec4a: Pull complete\n",
            "8fea5fa58717: Pull complete\n",
            "a73bd9480671: Pull complete\n",
            "139c41303dbe: Pull complete\n",
            "d7df78d5fe61: Pull complete\n",
            "7daa1f38b6ab: Pull complete\n",
            "bce9940ec07e: Pull complete\n",
            "01f79f64a163: Pull complete\n",
            "5113ced62272: Pull complete\n",
            "aa0eae780ea1: Pull complete\n",
            "caeb9ac3c484: Pull complete\n",
            "Digest: sha256:07e7ea696bf78f9e51c481f74c4cee100c00870cad74b04de849aea7e66f8c51\n",
            "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
            " ---> 2c270b121e81\n",
            "Step 2/8 : WORKDIR /root\n",
            " ---> Running in d130605792cb\n",
            "Removing intermediate container d130605792cb\n",
            " ---> 869d6b4991c9\n",
            "Step 3/8 : COPY requirements.txt .\n",
            " ---> 5df7fc34a117\n",
            "Step 4/8 : RUN pip3 install -U -r requirements.txt\n",
            " ---> Running in d85f4c52f38a\n",
            "Collecting tf-models-official==2.15.0 (from -r requirements.txt (line 1))\n",
            "  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.0.8)\n",
            "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (10.2.0)\n",
            "Collecting gin-config (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.118.0)\n",
            "Collecting immutabledict (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting kaggle>=1.3.9 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.7/82.7 kB 3.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (4.1.3)\n",
            "Collecting opencv-python-headless (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (5.9.8)\n",
            "Collecting py-cpuinfo>=3.3.0 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting pycocotools (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Collecting sacrebleu (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.8/51.8 kB 5.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Collecting sentencepiece (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting seqeval (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 5.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.16.0)\n",
            "Collecting tensorflow-datasets (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting tensorflow-hub>=0.6.0 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting tensorflow-text~=2.15.0 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow~=2.15.0 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tf-slim>=1.1.0 (from tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.10.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (4.66.2)\n",
            "Collecting python-slugify (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.2.0)\n",
            "Collecting bleach (from kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (16.0.6)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (23.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (69.0.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (4.9.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.29.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.60.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15.0->-r requirements.txt (line 1)) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15.0->-r requirements.txt (line 1)) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting regex (from sacrebleu->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.4.6)\n",
            "Collecting lxml (from sacrebleu->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1)) (8.1.7)\n",
            "Collecting promise (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1)) (15.0.0)\n",
            "Collecting simple-parsing (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting toml (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting array-record>=0.5.0 (from tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading array_record-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (692 bytes)\n",
            "Collecting etils>=1.6.0 (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading etils-1.11.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (0.42.0)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2024.2.0)\n",
            "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.56.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.15.0->-r requirements.txt (line 1)) (5.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.5.2)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting webencodings (from bleach->kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting text-unidecode>=1.3 (from python-slugify->kaggle>=1.3.9->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow-datasets->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1))\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15.0->-r requirements.txt (line 1)) (3.2.2)\n",
            "Downloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 43.3 MB/s eta 0:00:00\n",
            "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 475.2/475.2 MB 2.7 MB/s eta 0:00:00\n",
            "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.5/242.5 kB 25.3 MB/s eta 0:00:00\n",
            "Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 100.7 MB/s eta 0:00:00\n",
            "Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 352.1/352.1 kB 32.0 MB/s eta 0:00:00\n",
            "Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.3/61.3 kB 6.9 MB/s eta 0:00:00\n",
            "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
            "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.0/50.0 MB 40.6 MB/s eta 0:00:00\n",
            "Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 427.8/427.8 kB 30.2 MB/s eta 0:00:00\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.1/104.1 kB 11.8 MB/s eta 0:00:00\n",
            "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 68.7 MB/s eta 0:00:00\n",
            "Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 85.4 MB/s eta 0:00:00\n",
            "Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 14.0 MB/s eta 0:00:00\n",
            "Downloading array_record-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 87.8 MB/s eta 0:00:00\n",
            "Downloading etils-1.11.0-py3-none-any.whl (165 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 165.4/165.4 kB 18.6 MB/s eta 0:00:00\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 64.0 MB/s eta 0:00:00\n",
            "Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 84.4 MB/s eta 0:00:00\n",
            "Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 103.0 MB/s eta 0:00:00\n",
            "Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 442.0/442.0 kB 37.9 MB/s eta 0:00:00\n",
            "Downloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 76.0 MB/s eta 0:00:00\n",
            "Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 9.0 MB/s eta 0:00:00\n",
            "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.4/163.4 kB 18.1 MB/s eta 0:00:00\n",
            "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 99.5 MB/s eta 0:00:00\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.7/781.7 kB 43.3 MB/s eta 0:00:00\n",
            "Downloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.6/112.6 kB 13.6 MB/s eta 0:00:00\n",
            "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
            "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 58.9 MB/s eta 0:00:00\n",
            "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 103.9 MB/s eta 0:00:00\n",
            "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.2/78.2 kB 8.6 MB/s eta 0:00:00\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: kaggle, seqeval, promise\n",
            "  Building wheel for kaggle (setup.py): started\n",
            "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
            "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105786 sha256=48e8e75eee3b9a93b0562b9a55d797f150f69ed5790e45ceda50d3bef6cfc177\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/af/22/bf406f913dc7506a485e60dce8143741abd0a92a19337d83a3\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=0309e11c959a89ed6732380420d800abba1b3bb7d8af6eb03b13ae4574e42c85\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for promise (setup.py): started\n",
            "  Building wheel for promise (setup.py): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=016be3cfc177c0b9405de13e02db68d4c87a46fbfba00a03e4d13bba6c99427f\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
            "Successfully built kaggle seqeval promise\n",
            "Installing collected packages: webencodings, text-unidecode, sentencepiece, py-cpuinfo, gin-config, wrapt, toml, tensorflow-estimator, tensorboard-data-server, regex, python-slugify, protobuf, promise, portalocker, opencv-python-headless, ml-dtypes, lxml, keras, importlib_resources, immutabledict, etils, docstring-parser, bleach, absl-py, tf-slim, tensorflow-model-optimization, tensorflow-metadata, simple-parsing, sacrebleu, kaggle, seqeval, pycocotools, google-auth-oauthlib, tensorboard, array-record, tensorflow, tf-keras, tensorflow-datasets, tensorflow-hub, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.1\n",
            "    Uninstalling protobuf-3.20.1:\n",
            "      Successfully uninstalled protobuf-3.20.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.1.0\n",
            "    Uninstalling absl-py-2.1.0:\n",
            "      Successfully uninstalled absl-py-2.1.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "\u001b[0mSuccessfully installed absl-py-1.4.0 array-record-0.6.0 bleach-6.2.0 docstring-parser-0.16 etils-1.11.0 gin-config-0.5.0 google-auth-oauthlib-1.2.1 immutabledict-4.2.1 importlib_resources-6.5.2 kaggle-1.6.17 keras-2.15.0 lxml-5.3.0 ml-dtypes-0.3.2 opencv-python-headless-4.11.0.86 portalocker-3.1.1 promise-2.3 protobuf-3.20.3 py-cpuinfo-9.0.0 pycocotools-2.0.8 python-slugify-8.0.4 regex-2024.11.6 sacrebleu-2.5.1 sentencepiece-0.2.0 seqeval-1.2.2 simple-parsing-0.1.6 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.1 tensorflow-datasets-4.9.7 tensorflow-estimator-2.15.0 tensorflow-hub-0.16.1 tensorflow-metadata-1.16.1 tensorflow-model-optimization-0.8.0 tensorflow-text-2.15.0 text-unidecode-1.3 tf-keras-2.15.1 tf-models-official-2.15.0 tf-slim-1.1.0 toml-0.10.2 webencodings-0.5.1 wrapt-1.14.1\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0mRemoving intermediate container d85f4c52f38a\n",
            " ---> 4bd18537b796\n",
            "Step 5/8 : RUN rm -rf /var/sitecustomize/sitecustomize.py\n",
            " ---> Running in d5827db690f8\n",
            "Removing intermediate container d5827db690f8\n",
            " ---> 308b8f9cfc63\n",
            "Step 6/8 : COPY . /trainer\n",
            " ---> 2ff1620ebd84\n",
            "Step 7/8 : WORKDIR /trainer\n",
            " ---> Running in 2a213110210b\n",
            "Removing intermediate container 2a213110210b\n",
            " ---> 110b9892526a\n",
            "Step 8/8 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
            " ---> Running in ea8fd1dd9919\n",
            "Removing intermediate container ea8fd1dd9919\n",
            " ---> f41f05e289e3\n",
            "Successfully built f41f05e289e3\n",
            "Successfully tagged us-central1-docker.pkg.dev/qwiklabs-gcp-02-816e025829d7/diabetes-prediction-model/diabetes-prediction-model:latest\n",
            "PUSH\n",
            "Pushing us-central1-docker.pkg.dev/qwiklabs-gcp-02-816e025829d7/diabetes-prediction-model/diabetes-prediction-model:latest\n",
            "The push refers to repository [us-central1-docker.pkg.dev/qwiklabs-gcp-02-816e025829d7/diabetes-prediction-model/diabetes-prediction-model]\n",
            "a89f524389cf: Preparing\n",
            "ca767c73a5a0: Preparing\n",
            "a1539d7ca8c6: Preparing\n",
            "d89af66b6abd: Preparing\n",
            "e42695c7b436: Preparing\n",
            "e42695c7b436: Preparing\n",
            "7e34967c8575: Preparing\n",
            "03aa2a4bdb68: Preparing\n",
            "69ff3552dab2: Preparing\n",
            "bde9e2053036: Preparing\n",
            "bde9e2053036: Preparing\n",
            "b253aec57174: Preparing\n",
            "e9a5c35692b6: Preparing\n",
            "5ca5a09f80b2: Preparing\n",
            "f27306b95858: Preparing\n",
            "e96984247094: Preparing\n",
            "bf89224ff876: Preparing\n",
            "ca7739d6661c: Preparing\n",
            "ca7739d6661c: Preparing\n",
            "6afff9338181: Preparing\n",
            "5f70bf18a086: Preparing\n",
            "380cd88b9fb2: Preparing\n",
            "25c9ddea4aaa: Preparing\n",
            "eec152ec24b8: Preparing\n",
            "dd7d6ac03700: Preparing\n",
            "be9dc4e2456b: Preparing\n",
            "ceab7f116eb5: Preparing\n",
            "bd5ff18df433: Preparing\n",
            "a27f4aa3db94: Preparing\n",
            "1a102d1cac2b: Preparing\n",
            "7e34967c8575: Waiting\n",
            "03aa2a4bdb68: Waiting\n",
            "69ff3552dab2: Waiting\n",
            "bde9e2053036: Waiting\n",
            "b253aec57174: Waiting\n",
            "e9a5c35692b6: Waiting\n",
            "5ca5a09f80b2: Waiting\n",
            "f27306b95858: Waiting\n",
            "e96984247094: Waiting\n",
            "bf89224ff876: Waiting\n",
            "ca7739d6661c: Waiting\n",
            "6afff9338181: Waiting\n",
            "5f70bf18a086: Waiting\n",
            "380cd88b9fb2: Waiting\n",
            "25c9ddea4aaa: Waiting\n",
            "eec152ec24b8: Waiting\n",
            "dd7d6ac03700: Waiting\n",
            "be9dc4e2456b: Waiting\n",
            "ceab7f116eb5: Waiting\n",
            "bd5ff18df433: Waiting\n",
            "a27f4aa3db94: Waiting\n",
            "1a102d1cac2b: Waiting\n",
            "ca767c73a5a0: Pushed\n",
            "d89af66b6abd: Pushed\n",
            "a89f524389cf: Pushed\n",
            "e42695c7b436: Pushed\n",
            "7e34967c8575: Pushed\n",
            "69ff3552dab2: Pushed\n",
            "03aa2a4bdb68: Pushed\n",
            "bde9e2053036: Pushed\n",
            "e9a5c35692b6: Pushed\n",
            "b253aec57174: Pushed\n",
            "f27306b95858: Pushed\n",
            "ca7739d6661c: Pushed\n",
            "bf89224ff876: Pushed\n",
            "5f70bf18a086: Layer already exists\n",
            "6afff9338181: Pushed\n",
            "e96984247094: Pushed\n",
            "eec152ec24b8: Pushed\n",
            "25c9ddea4aaa: Pushed\n",
            "be9dc4e2456b: Pushed\n",
            "ceab7f116eb5: Pushed\n",
            "5ca5a09f80b2: Pushed\n",
            "bd5ff18df433: Pushed\n",
            "a27f4aa3db94: Pushed\n",
            "1a102d1cac2b: Pushed\n",
            "dd7d6ac03700: Pushed\n",
            "a1539d7ca8c6: Pushed\n",
            "380cd88b9fb2: Pushed\n",
            "latest: digest: sha256:1120f55ac07cfb28793a4c77c75a8c8ead849995954bcbcd311fefcb7206627e size: 6588\n",
            "DONE\n",
            "--------------------------------------------------------------------------------\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                                                                 STATUS\n",
            "141100e6-cbfe-4c78-893d-6c57d785d97e  2025-01-20T11:42:47+00:00  9M10S     gs://qwiklabs-gcp-02-816e025829d7_cloudbuild/source/1737373366.538596-2525e80fba04411881443e6c12dd98e0.tgz  us-central1-docker.pkg.dev/qwiklabs-gcp-02-816e025829d7/diabetes-prediction-model/diabetes-prediction-model (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "# use Cloud Build to build and submit the custom model container to Artifact Registry.\n",
        "!gcloud builds submit $MODEL_DIR \\\n",
        "    --config $MODEL_DIR/cloudbuild.yaml \\\n",
        "    --timeout=30m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cee35ac-ab83-472d-ab18-f622f3e3bc31",
      "metadata": {
        "id": "8cee35ac-ab83-472d-ab18-f622f3e3bc31"
      },
      "source": [
        "## Define a pipeline using the KFP V2 SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5556979-3583-44fd-86df-d30fab8d9464",
      "metadata": {
        "id": "c5556979-3583-44fd-86df-d30fab8d9464"
      },
      "source": [
        "To get higher performing model into production to deliver value faster, a pipeline is defined using the [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) to orchestrate the training and deployment of the model on [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines) below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef0e36b-3cb8-4660-bbb1-a5dcca49aebe",
      "metadata": {
        "id": "aef0e36b-3cb8-4660-bbb1-a5dcca49aebe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "# google_cloud_pipeline_components includes pre-built KFP components for interfacing with Vertex AI services.\n",
        "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
        "from kfp.v2 import dsl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95f7070-d6e5-47ab-a860-d6a7e7892164",
      "metadata": {
        "id": "c95f7070-d6e5-47ab-a860-d6a7e7892164",
        "outputId": "3c55eedc-dd0a-4021-94d4-95e06799c12d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model display name: diabetes-prediction-20250120115236\n",
            "GCS dir for model training artifacts: gs://qwiklabs-gcp-02-816e025829d7/diabetes-prediction-model-20250120115236\n",
            "GCS dir for pipeline artifacts: gs://qwiklabs-gcp-02-816e025829d7/pipeline_root/ZhaoYufan\n"
          ]
        }
      ],
      "source": [
        "TIMESTAMP=datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "DISPLAY_NAME = \"diabetes-prediction-{}\".format(TIMESTAMP)\n",
        "GCS_BASE_OUTPUT_DIR= f\"{GCS_BUCKET}/{MODEL_DIR}-{TIMESTAMP}\"\n",
        "\n",
        "USER = \"ZhaoYufan\"\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(GCS_BUCKET, USER)\n",
        "\n",
        "print(f\"Model display name: {DISPLAY_NAME}\")\n",
        "print(f\"GCS dir for model training artifacts: {GCS_BASE_OUTPUT_DIR}\")\n",
        "print(f\"GCS dir for pipeline artifacts: {PIPELINE_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4aecd42-c969-4ce0-a49a-9150c45a91e2",
      "metadata": {
        "id": "f4aecd42-c969-4ce0-a49a-9150c45a91e2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Pre-built Vertex model serving container for deployment.\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
        "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5e8dbc-04b3-4193-87f5-984d2b98a2d0",
      "metadata": {
        "id": "0f5e8dbc-04b3-4193-87f5-984d2b98a2d0"
      },
      "source": [
        "The pipeline consists of three components:\n",
        "\n",
        "* `CustomContainerTrainingJobRunOp` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp): trains the custom model container using Vertex Training.\n",
        "\n",
        "*  `EndpointCreateOp` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.EndpointCreateOp): Creates a Google Cloud Vertex Endpoint resource that maps physical machine resources with the model to enable it to serve online predictions. Online predictions have low latency requirements; providing resources to the model in advance reduces latency.\n",
        "\n",
        "* `ModelDeployOp`[(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.ModelDeployOp): deploys the model to a Vertex Prediction Endpoint for online predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2181f3d-10cd-49c8-8e2f-e5c314940321",
      "metadata": {
        "id": "d2181f3d-10cd-49c8-8e2f-e5c314940321",
        "tags": []
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name=\"diabetes-prediction\", pipeline_root=PIPELINE_ROOT)\n",
        "def pipeline(\n",
        "    project: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    staging_bucket: str = GCS_BUCKET,\n",
        "    display_name: str = DISPLAY_NAME,\n",
        "    container_uri: str = IMAGE_URI,\n",
        "    model_serving_container_image_uri: str = SERVING_IMAGE_URI,\n",
        "    base_output_dir: str = GCS_BASE_OUTPUT_DIR,\n",
        "):\n",
        "\n",
        "    # Add and configure the pre-built KFP CustomContainerTrainingJobRunOp component\n",
        "    model_train_evaluate_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
        "        # Vertex AI Python SDK authentication parameters.\n",
        "        project=project,\n",
        "        location=location,\n",
        "        staging_bucket=staging_bucket,\n",
        "        # WorkerPool arguments.\n",
        "        replica_count=1,\n",
        "        machine_type=\"e2-standard-4\",\n",
        "        display_name = display_name,\n",
        "        container_uri = container_uri,\n",
        "        model_serving_container_image_uri = model_serving_container_image_uri,\n",
        "        base_output_dir = base_output_dir,\n",
        "    )\n",
        "\n",
        "    # Create a Vertex Endpoint resource in parallel with model training.\n",
        "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
        "        # Vertex AI Python SDK authentication parameters.\n",
        "        project=project,\n",
        "        location=location,\n",
        "        display_name=display_name\n",
        "\n",
        "    )\n",
        "\n",
        "    # Deploy the model to the created Endpoint resource for online predictions.\n",
        "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
        "        # Link to model training component through output model artifact.\n",
        "        model=model_train_evaluate_op.outputs[\"model\"],\n",
        "        # Link to the created Endpoint.\n",
        "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
        "        # Define prediction request routing. {\"0\": 100} indicates 100% of traffic\n",
        "        # to the ID of the current model being deployed.\n",
        "        traffic_split={\"0\": 100},\n",
        "        # WorkerPool arguments.\n",
        "        dedicated_resources_machine_type=\"e2-standard-4\",\n",
        "        dedicated_resources_min_replica_count=1,\n",
        "        dedicated_resources_max_replica_count=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783114fd-731b-4bad-bbe2-7a858e621fca",
      "metadata": {
        "id": "783114fd-731b-4bad-bbe2-7a858e621fca"
      },
      "source": [
        "## Compile the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb28dac2-3721-4fe6-9e01-98745b0d1aba",
      "metadata": {
        "id": "eb28dac2-3721-4fe6-9e01-98745b0d1aba",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from kfp.v2 import compiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77355b83-577b-4831-9862-91e08e974256",
      "metadata": {
        "id": "77355b83-577b-4831-9862-91e08e974256",
        "outputId": "3a72d728-1f3d-4622-de7d-dbf2dc0f8192",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter/.local/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pipeline, package_path=\"diabetes-prediction.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793cda30-4046-4d29-abdd-501c243f5eee",
      "metadata": {
        "id": "793cda30-4046-4d29-abdd-501c243f5eee"
      },
      "source": [
        "## Run the pipeline on Vertex Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35be420d-9d1d-4e8e-a08a-658fdfd60eb0",
      "metadata": {
        "id": "35be420d-9d1d-4e8e-a08a-658fdfd60eb0"
      },
      "source": [
        "The `PipelineJob` is configured below and triggered through the `run()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f276575d-c2ba-4d08-9a2a-b7583af27aee",
      "metadata": {
        "id": "f276575d-c2ba-4d08-9a2a-b7583af27aee",
        "tags": []
      },
      "outputs": [],
      "source": [
        "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
        "    display_name=\"diabetes-prediction\",\n",
        "    template_path=\"diabetes-prediction.json\",\n",
        "    parameter_values={\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"location\": REGION,\n",
        "        \"staging_bucket\": GCS_BUCKET,\n",
        "        \"display_name\": DISPLAY_NAME,\n",
        "        \"container_uri\": IMAGE_URI,\n",
        "        \"model_serving_container_image_uri\": SERVING_IMAGE_URI,\n",
        "        \"base_output_dir\": GCS_BASE_OUTPUT_DIR},\n",
        "    enable_caching=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vertex_pipelines_job.run()"
      ],
      "metadata": {
        "id": "FRsdUPvg1Jn9"
      },
      "id": "FRsdUPvg1Jn9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "319a821a-a3bd-45bf-a9ea-aa18687218f6",
      "metadata": {
        "id": "319a821a-a3bd-45bf-a9ea-aa18687218f6"
      },
      "source": [
        "## Query the deployed model using the Vertex endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfd8366-d362-4537-ab30-21c21fce6846",
      "metadata": {
        "id": "6bfd8366-d362-4537-ab30-21c21fce6846"
      },
      "source": [
        "Finally, retrieve the `Endpoint` deployed by the pipeline and use it to query the model for online predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf80748b-8907-4ad6-8adb-d4c394752257",
      "metadata": {
        "id": "bf80748b-8907-4ad6-8adb-d4c394752257",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Retrieve the deployed Endpoint name from pipeline.\n",
        "ENDPOINT_NAME = vertexai.Endpoint.list()[0].name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67c989d-1026-4f57-8dac-dafad01145a8",
      "metadata": {
        "id": "c67c989d-1026-4f57-8dac-dafad01145a8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Generate online predictions using Vertex Endpoint.\n",
        "endpoint = vertexai.Endpoint(\n",
        "    endpoint_name=ENDPOINT_NAME,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97759f45-e060-44ce-87fc-4d34c4b8cadf",
      "metadata": {
        "id": "97759f45-e060-44ce-87fc-4d34c4b8cadf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "sample = {\n",
        "    'gender': 'Female',\n",
        "    'age': 56.0,\n",
        "    'hypertension': 1,\n",
        "    'heart_disease': 0,\n",
        "    'smoking_history': 'never',\n",
        "    'bmi': 27.53,\n",
        "    'HbA1c_level': 8.1,\n",
        "    'blood_glucose_level': 77,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = endpoint.predict(instances=[sample])"
      ],
      "metadata": {
        "id": "eEzEpzf-1S88"
      },
      "id": "eEzEpzf-1S88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54751a3e-7b2a-4ab8-b642-6533df27de82",
      "metadata": {
        "id": "54751a3e-7b2a-4ab8-b642-6533df27de82",
        "outputId": "16eae0a1-241d-4233-f9c7-f4a40f1905a4",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2.8715334]]\n"
          ]
        }
      ],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CpEf3GVD4seg",
      "metadata": {
        "id": "CpEf3GVD4seg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prob = tf.nn.sigmoid(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a4c68e-a937-44f8-b64a-cbe9e607cb90",
      "metadata": {
        "id": "c4a4c68e-a937-44f8-b64a-cbe9e607cb90",
        "outputId": "a2273624-27ec-4d26-fa9e-b044127295ad",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This particular patient had a 94.6 percent probability of being diabetic.\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"This particular patient had a %.1f percent probability \"\n",
        "    \"of being diabetic.\" % (100 * prob)\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m127",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}